{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Kernels in sci-kit learn - Lab"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this lab, you'll explore applying several types of kernels on some more visual data. At the end of the lab, you'll then apply your knowledge of SVMs to a real world dataset!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Objectives"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will be able to:\n", "- Create a non-linear SVM in scikit-learn\n", "- Interpret the results of your SVM in scikit-learn\n", "- Apply SVM to a real-world data set\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To start, reexamine the final datasets from the previous lab."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.datasets import make_blobs\n", "from sklearn.datasets import make_moons\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline  \n", "from sklearn import svm\n", "from sklearn.model_selection import train_test_split\n", "\n", "import numpy as np\n", "\n", "plt.figure(figsize=(10, 4))\n", "plt.subplot(121)\n", "plt.title(\"Four Blobs\")\n", "X_3, y_3 = make_blobs(n_samples=100, n_features=2, centers=4, cluster_std=1.6, random_state = 123)\n", "plt.scatter(X_3[:, 0], X_3[:, 1], c = y_3, s=25)\n", "\n", "plt.subplot(122)\n", "plt.title(\"Two Moons with Substantial Overlap\")\n", "X_4, y_4 = make_moons(n_samples=100, shuffle = False , noise = 0.3, random_state=123)\n", "plt.scatter(X_4[:, 0], X_4[:, 1], c = y_4, s=25)\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Explore the RBF kernel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Recall how a radial basis function kernel has 2 hyperparameters: `C` and `gamma`. To further investigate tuning, you'll generate 9 subplots with varying parameter values and plot the resulting decision boundaries. Take a look at this [example from sci-kit learn](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html) as inspiration. Each of the 9 plots should look like this:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["![](images/SVM_rbf.png)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Note that the score represents the percentage of correctly classified instances according to the model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a loop that builds a model for each of the 9 combinations\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "C_range =  np.array([0.1, 1, 10])  # [0.01, 10]\n", "gamma_range =  np.array([0.1, 1, 100]) # [1, 100] \n", "param_grid = dict(gamma=gamma_range, C=C_range)\n", "details = []\n", "for C in C_range:\n", "    for gamma in gamma_range:\n", "        clf = svm.SVC(C=C, gamma=gamma)\n", "        clf.fit(X_4, y_4)\n", "        details.append((C, gamma, clf))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Prepare your data for plotting\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false}, "outputs": [], "source": ["# Plot the prediction results in 9 subplots  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "# Plot the prediction results in 9 subplots  \n", "plt.figure(figsize=(11, 11))\n", "\n", "for (k, (C, gamma, clf)) in enumerate(details):\n", "    # evaluate the predictions in a grid\n", "    Z = clf.predict(x1x2)  \n", "    Z = Z.reshape(X1_C.shape)\n", "\n", "    # visualize decision function for these parameters\n", "    plt.subplot(3, 3, k + 1)\n", "    plt.title(\"gam= %r, C= %r, score = %r\"  % (gamma, C, round(clf.score(X_4,y_4), 2)))\n", "\n", "    # visualize parameter's effect on decision function\n", "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n", "    plt.scatter(X_4[:, 0], X_4[:, 1], c=y_4,  edgecolors='gray')\n", "    plt.axis('tight')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Repeat what you did before but now, use `decision_function` instead of `predict`. What do you see?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot the decision function results in 9 subplots\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Explore the Polynomial kernel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Recall that the polynomial kernel has 3 hyperparameters:\n", "- $\\gamma$, which can be specified using keyword `gamma`\n", "- $r$, which can be specified using keyword `coef0`\n", "- $d$, which can be specified using keyword `degree`"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build 8 different plots using all the possible combinations between there two values for each:\n", "- $r= 0.1$ and $2$\n", "- $\\gamma= 0.1$ and $1$\n", "- $d= 3$ and $4$\n", "\n", "Note that `decision_function()` cannot be used on a classifier with more than two classes, so simply use `predict()` again."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a loop that builds a model for each of the 8 combinations\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "r_range =  np.array([0.1, 2])  # [0.01, 10]\n", "gamma_range =  np.array([0.1, 1]) # [1, 100] \n", "d_range = np.array([3, 4])\n", "param_grid = dict(gamma=gamma_range, degree = d_range, coef0 = r_range)\n", "details = []\n", "for d in d_range:\n", "    for gamma in gamma_range:\n", "         for r in r_range:\n", "            clf = svm.SVC(kernel = \"poly\", coef0 = r , gamma=gamma, degree= d)\n", "            clf.fit(X_3, y_3)\n", "            details.append((r, d, gamma, clf))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Prepare your data for plotting\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "X1= X_3[:,0]\n", "X2= X_3[:,1]\n", "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n", "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n", "\n", "x1_coord = np.linspace(X1_min, X1_max, 500)\n", "x2_coord = np.linspace(X2_min, X2_max, 500)\n", "\n", "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n", "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot the prediction results in 8 subplots  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "plt.figure(figsize=(12, 14))\n", "\n", "for (k, (r, d,gamma, clf)) in enumerate(details):\n", "    # evaluate the predictions in a grid\n", "    Z = clf.predict(x1x2)  \n", "    Z = Z.reshape(X1_C.shape)\n", "\n", "    # visualize decision function for these parameters\n", "    plt.subplot(4, 2, k + 1)\n", "    plt.title(\"d= %r, gam= %r, r = %r , score = %r\"  % (d, gamma,r, round(clf.score(X_3,y_3), 2)))\n", "\n", "    # visualize parameter's effect on decision function\n", "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n", "    plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3,  edgecolors='gray')\n", "    plt.axis('tight')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## The Sigmoid Kernel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build a support vector machine using the Sigmoid kernel.\n", "\n", "Recall that the sigmoid kernel has 2 hyperparameters:\n", "- $\\gamma$, which can be specified using keyword `gamma`\n", "- $r$, which can be specified using keyword `coef0`\n", "\n", "\n", "Look at 9 solutions using the following values for $\\gamma$ and $r$.\n", "\n", "- $\\gamma= 0.001, 0.01$ and $0.1$\n", "- $r = 0.01, 1$ and $10$"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a loop that builds a model for each of the 9 combinations\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "r_range =  np.array([0.01, 1, 10])  \n", "gamma_range =  np.array([0.001, 0.01, 0.1]) \n", "param_grid = dict(gamma=gamma_range,coef0 = r_range)\n", "details = []\n", "for gamma in gamma_range:\n", "     for r in r_range:\n", "        clf = svm.SVC(kernel = \"sigmoid\", coef0 = r , gamma=gamma)\n", "        clf.fit(X_3, y_3) \n", "        details.append((r, gamma, clf))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Prepare your data for plotting\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "X1= X_3[:,0]\n", "X2= X_3[:,1]\n", "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n", "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n", "\n", "x1_coord = np.linspace(X1_min, X1_max, 500)\n", "x2_coord = np.linspace(X2_min, X2_max, 500)\n", "\n", "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n", "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot the prediction results in 9 subplots  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "plt.figure(figsize=(12, 14))\n", "\n", "for (k, (r, gamma, clf)) in enumerate(details):\n", "    # evaluate the predictions in a grid\n", "    Z = clf.predict(x1x2)  \n", "    Z = Z.reshape(X1_C.shape)\n", "\n", "    # visualize decision function for these parameters\n", "    plt.subplot(3, 3, k + 1)\n", "    plt.title(\" gam= %r, r = %r , score = %r\"  % (gamma,r, round(clf.score(X_3,y_3), 2)))\n", "\n", "    # visualize parameter's effect on decision function\n", "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n", "    plt.scatter(X_3[:, 0], X_3[:, 1], c=y_3,  edgecolors='gray')\n", "    plt.axis('tight')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## What is your conclusion here?\n", "\n", "- The polynomial kernel is very sensitive to the hyperparameter settings. Especially setting a \"wrong\" gamma can have a dramatic effect on the model performance\n", "- Our experiments with the Polynomial kernel were more successful"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Explore the Polynomial Kernels again, yet now performing a train-test-split"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Explore the same parameters you did before when exploring polynomial kernels\n", "- Do a train test split of 2/3 train vs 1/3 test. \n", "- Train the model on the training set, plot the result and theh accuracy score.\n", "- Next, plot the model with the test set and the resulting accuracy score. Make some notes for yourself on training vs test performance and selecting an appropriate model based on these results.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Perform a train test split, then create a loop that builds a model for each of the 8 combinations\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "X_train, X_test, y_train, y_test = train_test_split(X_3, y_3, test_size = 0.33, random_state=123)\n", "\n", "# Create a loop that builds a model for each of the 8 combinations\n", "r_range =  np.array([0.1, 2])  # [0.01, 10]\n", "gamma_range =  np.array([0.1, 1]) # [1, 100] \n", "d_range = np.array([3, 4])\n", "param_grid = dict(gamma=gamma_range, degree = d_range, coef0 = r_range)\n", "details = []\n", "for d in d_range:\n", "    for gamma in gamma_range:\n", "         for r in r_range:\n", "            clf = svm.SVC(kernel = \"poly\", coef0 = r , gamma=gamma, degree= d)\n", "            clf.fit(X_train, y_train)\n", "            details.append((r, d, gamma, clf))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Prepare your data for plotting\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot the prediction results in 8 subplots on the training set  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "# Plot the prediction results in 8 subplots  \n", "plt.figure(figsize=(12, 14))\n", "\n", "for (k, (r, d,gamma, clf)) in enumerate(details):\n", "    # evaluate the predictions in a grid\n", "    Z = clf.predict(x1x2)  \n", "    Z = Z.reshape(X1_C.shape)\n", "\n", "    # visualize decision function for these parameters\n", "    plt.subplot(4, 2, k + 1)\n", "    plt.title(\"d= %r, gam= %r, r = %r , score = %r\"  % (d, gamma,r, round(clf.score(X_train,y_train), 2)))\n", "\n", "    # visualize parameter's effect on decision function\n", "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n", "    plt.scatter(X1, X2, c=y_train,  edgecolors='gray')\n", "    plt.axis('tight')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Now plot the prediction results for the test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "# Prepare your data for plotting\n", "X1= X_test[:,0]\n", "X2= X_test[:,1]\n", "X1_min, X1_max = X1.min() - 1, X1.max() + 1\n", "X2_min, X2_max = X2.min() - 1, X2.max() + 1\n", "\n", "x1_coord = np.linspace(X1_min, X1_max, 500)\n", "x2_coord = np.linspace(X2_min, X2_max, 500)\n", "\n", "X2_C, X1_C = np.meshgrid(x2_coord, x1_coord)\n", "x1x2 = np.c_[X1_C.ravel(), X2_C.ravel()]\n", "\n", "# Plot the prediction results in 8 subplots  \n", "plt.figure(figsize=(12, 14))\n", "\n", "for (k, (r, d,gamma, clf)) in enumerate(details):\n", "    # evaluate the predictions in a grid\n", "    Z = clf.predict(x1x2)  \n", "    Z = Z.reshape(X1_C.shape)\n", "\n", "    # visualize decision function for these parameters\n", "    plt.subplot(4, 2, k + 1)\n", "    plt.title(\"d= %r, gam= %r, r = %r , score = %r\"  % (d, gamma,r, round(clf.score(X_test,y_test), 2)))\n", "\n", "    # visualize parameter's effect on decision function\n", "    plt.contourf(X1_C, X2_C, Z, alpha = 1)\n", "    plt.scatter(X1, X2, c=y_test,  edgecolors='gray')\n", "    plt.axis('tight')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## A higher-dimensional, real world data set"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Until now, you've only explored data sets with 2 features to make it easy to visualize the decision boundary. Remember that you can use Support Vector Machines on a wide range of classification data sets, with more than 2 features. While you will no longer be able to visually represent decision boundaries (at least, if you have more than 3 feature spaces), you'll still be able to make predictions.\n", "\n", "To do this, you'll use the salaries dataset again (in `salaries_final.csv`). \n", "\n", "This dataset has 6 predictors:\n", "\n", "- `Age`: continuous.\n", "\n", "- `Education`: Categorical. Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n", "\n", "- `Occupation`: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n", "\n", "- `Relationship`: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n", "\n", "- `Race`: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n", "\n", "- `Sex`: Female, Male.\n", "\n", "Simply run the code below to import and preview the dataset. Be sure to note the data type produced by`dmatrices`. `dmatrices` is often used for preprocessing data with continuous and categorical predictors."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import statsmodels as sm\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.model_selection import train_test_split\n", "import pandas as pd\n", "salaries = pd.read_csv(\"salaries_final.csv\", index_col = 0)\n", "salaries.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["target = pd.get_dummies(salaries.Target, drop_first=True)\n", "xcols = salaries.columns[:-1]\n", "data = pd.get_dummies(salaries[xcols], drop_first=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now build a simple linear SVM using this data. Note that using SVC, some slack is automatically allowed, so the data doesn't have to perfectly linearly separable.\n", "\n", "- Create a train-test-split of 75-25\n", "- Standardize the data\n", "- Fit an SVM model, make sure that you set \"probability = True\"\n", "- After you run the model, calculate the classification accuracy score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Split the data into a train and test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "X_train, X_test, y_train, y_test = train_test_split(data, target, \n", "                                                    test_size = 0.25, random_state=123)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Standardize the data\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "\n", "# Standardize the data\n", "std = StandardScaler()\n", "X_train_transformed = std.fit_transform(X_train)\n", "X_test_transformed = std.transform(X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Fit the SVM model. This will take some time!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "# Fit SVM model. This will take some time!\n", "clf = svm.SVC(probability=True)\n", "clf.fit(X_train_transformed, y_train['>50K'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Calculate the classification accuracy score"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#__SOLUTION__\n", "clf.score(X_test_transformed,y_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["> Warning: It takes quite a while to compute this! The score is slightly better than the best result obtained using decision trees, but at the cost of computational resources. Changing kernels can make computation times even longer."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great, you've got plenty of practice in on Support Vector Machines! In this lab you explored kernels and applying SVMs to real-life data!"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.8"}}, "nbformat": 4, "nbformat_minor": 2}