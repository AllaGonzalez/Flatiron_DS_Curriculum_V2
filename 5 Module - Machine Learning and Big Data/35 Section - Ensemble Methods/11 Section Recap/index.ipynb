{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this section, you learned about ensemble methods. This lesson will summarize the key takeaways from this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "You will be able to:\n",
    "* Understand and explain what was covered in this section\n",
    "* Understand and explain why this section will help you become a data scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "The key takeaways from this section include:\n",
    "* The delphi technique suggests that multiple independent estimates will be more consistently accurate than any single estimate\n",
    "* Because of this, ensemble techniques are a powerful way for improving the quality of your models\n",
    "* Sometimes you'll use model stacking or meta-ensembles where you use of combination of different types of model for your ensemble\n",
    "* It's also common to have multiple similar models in an ensemble - e.g. a bunch of decision trees\n",
    "* Bagging (Bootstrap AGGregation) is a technique that leverages Bootstrap Resampling and Aggregation\n",
    "* Bootstrap resampling uses multiple smaller samples from the test data set to create independent estimates, and aggregation is the combining of those estimates to make predictions\n",
    "* A random forest is an ensemble method for decision trees using Bagging and the Subspace Sampling Method to create variance among the trees\n",
    "* With a random forest, for each tree, we sample using 2/3 of the training data and the remaining third is used to calculate the Out-of-Bag Error\n",
    "* In addition, the Subspace Sampling Method is used to further increase variability by randomly selecting the subset of features to use as predictors for training any given tree\n",
    "* GridsearchCV is an exhaustive search technique for finding optimal combinations of hyper parameters by calculating for every combination of parameter values you put into the search\n",
    "* Gradient boosting leverages an ensemble of weak learners (weak models) to create a strong combined model\n",
    "* Boosting (when compared to random forests) is an iterative rather than independent process, using each model to strengthen the weaknesses of the previous ones\n",
    "* Two of the most common algorithms for Boosting are Adaboost (Adaptive Boosting) and Gradient Boosted Trees\n",
    "* Adaboost creates new classifiers by continually influencing the distribution of the data sampled to train each successive tree\n",
    "* Gradient Boosted Trees are a more advanced boosting algorithm that makes use of Gradient Descent\n",
    "* XGBoost (eXtreme Gradient Boosting) is one of the top gradient boosting algorithms currently in use\n",
    "* XGBoost is a stand-alone library that implements popular gradient boosting algorithms in the fastest, most performant way possible\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
