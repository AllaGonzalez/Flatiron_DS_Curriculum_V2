
# Section Recap

## Introduction

Congratulations, you've just modeled some complex relationships with interaction terms, polynomials, and regularization! Here is a recap of what you learned in this section.

## Objectives
You will be able to:
* Critically think about the different ways data scientists add complexity to basic regression models.
* Determine the right balance between complexity and simplicity through the lens of bias and variance

## Key Takeaways

This section gave you the chance to learn about techniques whereby you can model non-linear relationships with data. It's very rare that real-world problems can be modeled with a simple linear regression, so it's important to get yourself well acquainted with creating new features and selecting the most important ones.

* An interaction is a particular property of three or more variables, where two or more variables interact in a non-additive manner when affecting a third variable.
* Polynomial regression allows for better fitting data that isn't well predicted using a linear model.
* The risk of polynomial regressions is that it's easier to overfit data, so it's important to consider the Bias-Variance trade-off and perform proper cross validation.
* Ridge and Lasso regressions are two regularization techniques used for making complex models more expensive in the cost function, reducing the risk of overfitting.
* Feature Selection is an important component of model building, and it can have a drastic impact on the overall performance of a model.
* AIC and BIC are techniques for selecting models that penalize models the more complex they are.

## Summary

Excellent work! You learned a substantial amount about different ways to model non-linear relationships. You will continue to use and build upon the concepts learned in this section for the rest of your machine learning career.
