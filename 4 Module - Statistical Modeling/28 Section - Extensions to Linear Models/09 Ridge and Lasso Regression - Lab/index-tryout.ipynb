{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you'll practice your knowledge on Ridge and Lasso regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Lasso and ridge regression in Python\n",
    "- Compare Lasso and Ridge with standard regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall our cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previously, you know that when solving for a linear regression, you can express the cost function as\n",
    "\n",
    "This is the expression for simple linear regression (for 1 predictor $x$). If you have multiple predictors, you would have something that looks like:\n",
    "\n",
    "$$ \\text{cost_function}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2$$\n",
    "\n",
    "where $k$ is the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ridge regression, the linear regression cost function is changed by adding a penalty term to square of the magnitude of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function_ridge}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p m_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{cost_function_lasso}= \\sum_{i=1}^n(y_i - \\hat{y})^2 = \\sum_{i=1}^n(y_i - \\sum_{j=1}^k(m_jx_{ij} + b))^2 + \\lambda \\sum_{j=1}^p \\mid m_j \\mid$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, for our gradients, when having multiple predictors $x_j$ with $j \\in 1,\\ldots, k$\n",
    "\n",
    "$$ \\frac{dJ}{dm_j}J(m_j,b) = -2\\sum_{i = 1}^n x_{j,i}(y_i - (\\sum_{j=1}^km_j{x_{ij}} + b)) = -2\\sum_{i = 1}^n x_{ij}*\\epsilon_i$$\n",
    "$$ \\frac{dJ}{db}J(m_j,b) = -2\\sum_{i = 1}^n(y_i - (\\sum_{j=1}^km_j{x_{ij}} + b)) = -2\\sum_{i = 1}^n \\epsilon_i $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example using our `auto-mpg` data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform our continuous predictors in `auto-mpg` and see how they perform as predictors in a Ridge versus Lasso regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617571</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.536150</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>0.646739</td>\n",
       "      <td>0.589736</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.645995</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.516870</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.516019</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.604651</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.520556</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1         2         3         4         5    6\n",
       "0  18.0  1.0  0.617571  0.456522  0.536150  0.238095  0.0\n",
       "1  15.0  1.0  0.728682  0.646739  0.589736  0.208333  0.0\n",
       "2  18.0  1.0  0.645995  0.565217  0.516870  0.178571  0.0\n",
       "3  16.0  1.0  0.609819  0.565217  0.516019  0.238095  0.0\n",
       "4  17.0  1.0  0.604651  0.510870  0.520556  0.148810  0.0"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(\"auto-mpg.csv\") \n",
    "data['horsepower'].astype(str).astype(int)\n",
    "y = data[[\"mpg\"]]\n",
    "X = data.drop([\"mpg\", \"car name\", \"origin\"], axis=1)\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "transformed = scale.fit_transform(X)\n",
    "X = pd.DataFrame(transformed, columns = X.columns)\n",
    "\n",
    "data= pd.concat([y,X], axis=1, ignore_index= True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we created train-test-splits, and created Ridge, Lasso and Linear regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform test train split\n",
    "X_train , X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.concat([y_train,X_train], axis=1, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.reset_index(drop=True).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current ,points):\n",
    "    b_gradient = 0\n",
    "    m_gradient = np.zeros(len(m_current))\n",
    "    learning_rate = .01\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        y = points[i][0]\n",
    "        x = points[i][1:(len(m_current)+1)] \n",
    "        b_gradient += -(1/N)  * (y -  (sum(m_current * x) + b_current))\n",
    "        m_gradient += -(1/N) * x * (y -  (sum(m_current * x) + b_current))\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_m = m_current - (learning_rate * m_gradient)\n",
    "    return (new_b, new_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0\n",
    "m = [0,0,0,0,0,0]\n",
    "updated_b, updated_m = step_gradient(b, m, data_train) # {'b': 0.0085, 'm': 0.6249999999999999}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our initial step with m and b values, and the corresponding error.\n",
    "b = 0\n",
    "m = [0,0,0,0,0,0]\n",
    "iterations = []\n",
    "for i in range(5000):\n",
    "    iteration = step_gradient(b, m, data_train)\n",
    "    b= iteration[0]\n",
    "    m = []\n",
    "    for j in range(len(iteration[1])):\n",
    "        m.append(iteration[1][j+1])\n",
    "    iterations.append(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.746290723761629, 1     1.571408\n",
       " 2    -0.922501\n",
       " 3    -0.640380\n",
       " 4     0.519102\n",
       " 5     7.499974\n",
       " 6    10.899802\n",
       " Name: 0, dtype: float64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations[4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a Ridge, Lasso and regular linear regression model. \n",
    "# Note how in scikit learn, the regularization parameter is denoted by alpha (and not lambda)\n",
    "ridge = Ridge(alpha=0.5)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "lin = LinearRegression()\n",
    "lin.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create predictions for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preditions for training and test sets\n",
    "y_h_ridge_train = ridge.predict(X_train)\n",
    "y_h_ridge_test = ridge.predict(X_test)\n",
    "\n",
    "y_h_lasso_train = np.reshape(lasso.predict(X_train), (274,1))\n",
    "y_h_lasso_test = np.reshape(lasso.predict(X_test), (118,1))\n",
    "\n",
    "y_h_lin_train = lin.predict(X_train)\n",
    "y_h_lin_test = lin.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the RSS for train and test for each of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error Ridge Model mpg    2688.222824\n",
      "dtype: float64\n",
      "Test Error Ridge Model mpg    2074.197775\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Lasso Model mpg    4644.536425\n",
      "dtype: float64\n",
      "Test Error Lasso Model mpg    3696.183375\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Train Error Unpenalized Linear Model mpg    2658.043444\n",
      "dtype: float64\n",
      "Test Error Unpenalized Linear Model mpg    1976.266987\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('Train Error Ridge Model', np.sum((y_train - y_h_ridge_train)**2))\n",
    "print('Test Error Ridge Model', np.sum((y_test - y_h_ridge_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Lasso Model', np.sum((y_train - y_h_lasso_train)**2))\n",
    "print('Test Error Lasso Model', np.sum((y_test - y_h_lasso_test)**2))\n",
    "print('\\n')\n",
    "\n",
    "print('Train Error Unpenalized Linear Model', np.sum((y_train - lin.predict(X_train))**2))\n",
    "print('Test Error Unpenalized Linear Model', np.sum((y_test - lin.predict(X_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that Ridge is clearly better than Lasso here, but that the unpenalized model performs best here. Let's see how including Ridge and Lasso changed our parameter estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge parameter coefficients: [[ -2.11792413  -3.0112953   -1.90579654 -15.60758962  -1.61071692\n",
      "    8.12940111]]\n",
      "Lasso parameter coefficients: [-10.31005725  -0.          -0.          -2.27967948   0.\n",
      "   3.88327477]\n",
      "Linear model parameter coefficients: [[ -1.33790698  -1.05300843  -0.08661412 -20.08143923  -0.39639115\n",
      "    8.56051229]]\n"
     ]
    }
   ],
   "source": [
    "print('Ridge parameter coefficients:', ridge.coef_)\n",
    "print('Lasso parameter coefficients:', lasso.coef_)\n",
    "print('Linear model parameter coefficients:', lin.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see how Lasso shrinks certain parameters to 0! The Ridge regression mostly affected the fourth parameter (estimated to be -20.08 for the linear regression model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional reading\n",
    "\n",
    "Full code examples for Ridge and Lasso regression, advantages and disadvantages, and how to code ridge and Lasso in Python can be found [here](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/).\n",
    "\n",
    "Make sure to have a look at the Scikit-Learn documentation for [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) and [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You now know how to perform Lasso and Ridge regression. Let's move on to the lab to explore Lasso and Ridge further!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
