{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE Review\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You've seen Maximum Likelihood Estimation when discussing Bayesian statistics, but did you know logistic regression can also be seen from this statistical perspective? In this lesson, you'll gain a deeper understanding of logistic regression by coding it from scratch and analyzing the statistical motivations backing it. With that, take a minute to review maximum likelihood estimation.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Understand and explain the MLE in terms of maximizing likelihood/log likelihood vs. minimizing negative log likelihood\n",
    "\n",
    "## MLE\n",
    "\n",
    "Maximum likelihood estimation can often sound, academic, confusing and cryptic when first introduced. It is often presented and introduced with complex integrals of statistical distributions that scare away many readers. Hopefully, this hasn't been your experience to date. While the mathematics can become complex quickly, the underlying concepts are actually quite intuitive.\n",
    "\n",
    "To demonstrate this, imagine a simple coin flipping example. Let's say that you flip a coin 100 times and get 55 heads. Maximum likelihood estimation attempts to uncover the underlying theoretical probability of this coin landing on heads given your observations. In other words, given the observations, what is the chance that the coin was fair and had a .5 chance of landing on heads each time? Or what is the chance that the coin actually had a .75 probability of lands of heads, given what we observed? It turns out that the answer to these questions is rather intuitive. If you observe 55 out of 100 coin flips, the underlying probability which maximizes the chance of us observing 55 out of 100 coin flips is .55. In this simple example, MLE simply returns the current sample mean as the underlying parameter that makes the observations most probable. Slight deviations to this would be almost as probable but slightly less so, and large deviations from our sample mean should be rare. This intuitively make some sense; as your sample size increases, you expect the sample mean to converge to the true underlying parameter. MLE takes a flipped perspective, asking what underlying parameter is most probable given the observations.\n",
    "\n",
    "## Log Likelihood\n",
    "\n",
    "When calculating maximum likelihood, it is common to use the log likelihood, as taking the logarithm can simplify calculations. For example, taking the logarithm of a set of products allows you to decompose the problem from products into sums. (You may recall from high school mathematics that $x^{(a+b)} = x^a \\bullet x^b$. Similarly, taking the logarithm of both sides of a function allows you to transform products into sums. \n",
    "\n",
    "## MLE for a Binomial Variable\n",
    "\n",
    "Let's take a deeper mathematical investigation into the coin flipping example above. \n",
    "\n",
    "In general, if you were to observe n flips, you would have observations $y_1, y_2, ..., y_n$.\n",
    "\n",
    "In maximum likelihood estimation, you are looking to maximize the likelihood:  \n",
    "\n",
    "$L(p) = L(y_1, y_2, ..., y_n | p) = p^y (1-p)^{n-y}$  where $ y = \\sum_{i=1}^{n}y_i$\n",
    "\n",
    "Taking the log of both sides:  \n",
    "\n",
    "$ln[L(p)] = ln[p^y (1-p)^{n-y}] = y ln(p)+(n-y)ln(1-p)$\n",
    "\n",
    "If y = 1,2,...,n-1 the derivative of ln[L(p)] with respect to p is:\n",
    "\n",
    "$\\frac{d\\,ln[L(p)]}{dp} = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})$  \n",
    "\n",
    "As you've seen previously, the maximum will then occur when the derivative equals zero:  \n",
    "\n",
    "$0 = y (\\frac{1}{p})+(n-y)(\\frac{-1}{1-p})$\n",
    "\n",
    "Distributing, you have\n",
    "\n",
    "$0 = \\frac{y}{p} - \\frac{n-y}{1-p}$\n",
    "\n",
    "And solving for p: \n",
    "\n",
    "$ \\frac{n-y}{1-p} = \\frac{y}{p} $\n",
    "\n",
    "$p(n-y) = \\frac{y(1-p)}{p}$  \n",
    "$\\frac{n-y}{y} = \\frac{1-p}{p}$  \n",
    "$\\frac{n}{y}-1 = \\frac{1}{p}-1$  \n",
    "$\\frac{n}{y} = \\frac{1}{p} $  \n",
    "$p = \\frac{y}{n}$  \n",
    "\n",
    "And voil√†, you've verified the intuitive solution discussed above; the maximum likelihood for a binomial sample is the observed frequency!\n",
    " \n",
    "## Additional Resources\n",
    "\n",
    "For more review on MLE, take a look back at some of our previous lessons [here](https://github.com/learn-co-curriculum/dsc-2-21-11-PE-MLE).\n",
    "## Summary\n",
    "\n",
    "In this lesson you briefly reviewed maximum likelihood estimation. In the upcoming lesson, you'll see how logistic regression can also be interpreted from this framework, which will help set the stage for you to code a logistic regression function from scratch using NumPy. Continue on to the next lesson to take a look at how this works for logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
