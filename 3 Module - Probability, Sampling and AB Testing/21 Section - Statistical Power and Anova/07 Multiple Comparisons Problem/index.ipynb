{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Multiple Comparisons Problem\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, you'll learn about the problems that can arise from doing multiple comparisons in a single experiment.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Understand and explain the concept of spurious correlation\n",
    "* Understand and explain why multiple comparisons increases the likelihood of misleading results\n",
    "* Understand and use corrections such as the Bonferroni Correction to deal with multiple comparisons\n",
    "\n",
    "\n",
    "## What is the Multiple Comparisons Problem?\n",
    "\n",
    "Obtaining an incredibly low p-value does not guarantee that the null-hypothesis is incorrect. For example, a p-value of 0.001 states that there is still a 1 in 1000 chance that the null hypothesis is true. Yet, as you've seen, p-values alone can be misleading. For example, if you perform repeated experiments, at some point you're apt to stumble upon a small p-value, whether or not the null hypothesis is valid.\n",
    "\n",
    "To restate this, imagine we take 100 scientific studies with a p-values of .03. Are all of these conclusions valid? Sadly, probably not. Remember, for any experiment with a p-value of .03, there is still a 3% chance that the null-hypothesis is actually true. So collectively, the probability that **all** of these null hypotheses are false is actually quite small. You can be fairly confident in each study, but there is also apt to be a false-conclusion drawn somewhere. (In fact the p-value itself implies that, on average, 3 of these 100 conclusions will be false.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04755250792540563"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".97**100 #Probability all 100 experiments with p=0.03 are all true "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if you are testing multiple metrics simultaneously in an experiment, the chances that one of these will satisfy your alpha threshold increases. A fun similar phenomenon is spurious correlation. If we start comparing a multitude of quantities, we are bound to find some that are highly correlated, whether or not an actual relationship exists. Tyler Vigen set out to find such relationships; here are several entertaining one's (of many):  \n",
    "\n",
    "<img src=\"images/nicolas_cage_vs_drowning.svg\">\n",
    "<img src=\"images/chicken_vs_oil.svg\">\n",
    "<img src=\"images/math_phds_vs_uranium.svg\">\n",
    "<img src=\"images/spelling_vs_spiders.svg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As we can see, although these graphs show that each of pair of quantities are strongly correlated, it seems unreasonable to expect that any of them have any causal relationships. Regardless of what the statistics tells us, there is no relationship through which the length of spelling bee word affects then number of people killed by venomous spiders.\n",
    "\n",
    "\n",
    "## How do Multiple Comparisons Increase the Chances of Finding Spurious Correlations?\n",
    "\n",
    "Spurious correlation is a **_Type 1 Error_**, meaning that it's a type of **_False Positive_**. We think we've found something important, when really there isn't.  With each comparison we make in an experiment, we try to set a really low p-value to limit our exposure to type 1 errors.  When we only reject the null hypothesis when p < 0.05, for example, we are effectively saying \"I'm only going to accept these results as true if there is less than a 5% chance that I didn't actually find anything important, and my data only looks like this due to randomness\".  However, when we make **_Multiple Comparisons_** by checking for many things at once, each of small risks of a Type 1 Error become cumulative! \n",
    "\n",
    "Here's another easy to way to phrase this--a p-value threshold of < .05 means that we will only make a Type 1 error 1 in every 20 times. This means that statistically, if I have 20 findings where my p-value is less than < .05 at the same time, 1 of them is almost guaranteed to be a Type 1 error (False Positive)--but I have no idea of which one!\n",
    "\n",
    "## The Bonferroni Correction\n",
    "\n",
    "Back to the problem of multiple comparisons. Due to the cumulative risk of drawing false conclusions when statistically testing multiple quantities simultaneously, statisticians have devised methods to minimize the chance of type I errors. One of these is the **_Bonferroni Correction_**.  With the Bonferroni correction, you divide $\\alpha$ by the number of comparisons you are making to set a new, adjusted threshold rejecting the null hypothesis.\n",
    "\n",
    "For example, if you desire $\\alpha = 0.05$, but are making 10 comparisons simultaneously, the Bonferroni Correction would advise you set an our adjusted p-value threshold to $\\frac{0.05}{10} = 0.005$!  The stricter p-value threshold helps control for Type 1 errors.  This doesn't mean that you are immune to them--it just helps reduce the cumulative chance that one occurs. That said, the effective power of these tests is therefore reduced (and in turn type II errors are more likely).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [Tyle Vigen - Spurious Correlations](http://tylervigen.com/spurious-correlations)\n",
    "* [Nick Cage Movies Vs. Drownings, and More Strange (but Spurious) Correlations](https://www.nationalgeographic.com/science/phenomena/2015/09/11/nick-cage-movies-vs-drownings-and-more-strange-but-spurious-correlations/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, you learned about the problems that can arise from doing multiple comparisons in a single experiment, as well as some entertaining spurious correlations that exist with real world data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
