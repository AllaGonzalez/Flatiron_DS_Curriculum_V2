{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\"Parameter Inference\" is one of the most important concepts of predictive machine learning. In this lesson, you will begin to build an intuition surrounding the the ideas around this concept. You'll first look at the maximum likelihood estimation (MLE) for the posterior probability based on observed data. (A direct application of Bayes Theorem.) From there, you'll conduct a random experiment involving a series of coin tosses to derive the general formula for MLE of a binomial distribution. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "* Understand and describe parametric inference based in context of identifying optimal values for model parameters\n",
    "* Understand the concept of likelihood , and the difference between likelihood and a probability\n",
    "* Describe MLE assumption of IID samples and its implications on model development\n",
    "\n",
    "## Parameter Inference\n",
    "\n",
    "Parameter Inference is the process of probabilistically inferring parameter(s) for a model of our choice, that is which parameter values best describe the underlying dataset, used in an analytical context. Let's try to understand this with a simple experiment with a 10 times coin flip and inspecting the outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def coinToss():\n",
    "    number = int(input(\"Number of times to flip coin: \"))\n",
    "    recordList = []\n",
    "    heads = 0\n",
    "    tails = 0\n",
    "    for amount in range(number):\n",
    "         flip = random.randint(0, 1)\n",
    "         if (flip == 0):\n",
    "              print(\"Toss\", amount+1 ,':' , \"Heads\")\n",
    "              recordList.append(\"Heads\")\n",
    "         else:\n",
    "              print(\"Toss\", amount+1 ,':' , \"Tails\")\n",
    "              recordList.append(\"Tails\")\n",
    "    print(str(recordList))\n",
    "    print(str(recordList.count(\"Heads\")) + str(recordList.count(\"Tails\")))\n",
    "    return recordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times to flip coin: 100\n",
      "Toss 1 : Tails\n",
      "Toss 2 : Heads\n",
      "Toss 3 : Tails\n",
      "Toss 4 : Tails\n",
      "Toss 5 : Heads\n",
      "Toss 6 : Tails\n",
      "Toss 7 : Heads\n",
      "Toss 8 : Heads\n",
      "Toss 9 : Heads\n",
      "Toss 10 : Tails\n",
      "Toss 11 : Heads\n",
      "Toss 12 : Heads\n",
      "Toss 13 : Heads\n",
      "Toss 14 : Heads\n",
      "Toss 15 : Heads\n",
      "Toss 16 : Heads\n",
      "Toss 17 : Heads\n",
      "Toss 18 : Tails\n",
      "Toss 19 : Heads\n",
      "Toss 20 : Tails\n",
      "Toss 21 : Heads\n",
      "Toss 22 : Tails\n",
      "Toss 23 : Tails\n",
      "Toss 24 : Heads\n",
      "Toss 25 : Tails\n",
      "Toss 26 : Heads\n",
      "Toss 27 : Heads\n",
      "Toss 28 : Heads\n",
      "Toss 29 : Heads\n",
      "Toss 30 : Tails\n",
      "Toss 31 : Heads\n",
      "Toss 32 : Heads\n",
      "Toss 33 : Tails\n",
      "Toss 34 : Tails\n",
      "Toss 35 : Heads\n",
      "Toss 36 : Heads\n",
      "Toss 37 : Tails\n",
      "Toss 38 : Tails\n",
      "Toss 39 : Heads\n",
      "Toss 40 : Heads\n",
      "Toss 41 : Tails\n",
      "Toss 42 : Tails\n",
      "Toss 43 : Heads\n",
      "Toss 44 : Tails\n",
      "Toss 45 : Tails\n",
      "Toss 46 : Heads\n",
      "Toss 47 : Tails\n",
      "Toss 48 : Tails\n",
      "Toss 49 : Heads\n",
      "Toss 50 : Heads\n",
      "Toss 51 : Heads\n",
      "Toss 52 : Tails\n",
      "Toss 53 : Heads\n",
      "Toss 54 : Heads\n",
      "Toss 55 : Heads\n",
      "Toss 56 : Tails\n",
      "Toss 57 : Heads\n",
      "Toss 58 : Heads\n",
      "Toss 59 : Tails\n",
      "Toss 60 : Heads\n",
      "Toss 61 : Tails\n",
      "Toss 62 : Tails\n",
      "Toss 63 : Heads\n",
      "Toss 64 : Heads\n",
      "Toss 65 : Tails\n",
      "Toss 66 : Heads\n",
      "Toss 67 : Tails\n",
      "Toss 68 : Tails\n",
      "Toss 69 : Heads\n",
      "Toss 70 : Heads\n",
      "Toss 71 : Heads\n",
      "Toss 72 : Tails\n",
      "Toss 73 : Heads\n",
      "Toss 74 : Tails\n",
      "Toss 75 : Heads\n",
      "Toss 76 : Tails\n",
      "Toss 77 : Heads\n",
      "Toss 78 : Tails\n",
      "Toss 79 : Tails\n",
      "Toss 80 : Tails\n",
      "Toss 81 : Heads\n",
      "Toss 82 : Tails\n",
      "Toss 83 : Tails\n",
      "Toss 84 : Heads\n",
      "Toss 85 : Tails\n",
      "Toss 86 : Heads\n",
      "Toss 87 : Heads\n",
      "Toss 88 : Heads\n",
      "Toss 89 : Heads\n",
      "Toss 90 : Heads\n",
      "Toss 91 : Heads\n",
      "Toss 92 : Heads\n",
      "Toss 93 : Tails\n",
      "Toss 94 : Heads\n",
      "Toss 95 : Tails\n",
      "Toss 96 : Tails\n",
      "Toss 97 : Tails\n",
      "Toss 98 : Tails\n",
      "Toss 99 : Tails\n",
      "Toss 100 : Tails\n",
      "['Tails', 'Heads', 'Tails', 'Tails', 'Heads', 'Tails', 'Heads', 'Heads', 'Heads', 'Tails', 'Heads', 'Heads', 'Heads', 'Heads', 'Heads', 'Heads', 'Heads', 'Tails', 'Heads', 'Tails', 'Heads', 'Tails', 'Tails', 'Heads', 'Tails', 'Heads', 'Heads', 'Heads', 'Heads', 'Tails', 'Heads', 'Heads', 'Tails', 'Tails', 'Heads', 'Heads', 'Tails', 'Tails', 'Heads', 'Heads', 'Tails', 'Tails', 'Heads', 'Tails', 'Tails', 'Heads', 'Tails', 'Tails', 'Heads', 'Heads', 'Heads', 'Tails', 'Heads', 'Heads', 'Heads', 'Tails', 'Heads', 'Heads', 'Tails', 'Heads', 'Tails', 'Tails', 'Heads', 'Heads', 'Tails', 'Heads', 'Tails', 'Tails', 'Heads', 'Heads', 'Heads', 'Tails', 'Heads', 'Tails', 'Heads', 'Tails', 'Heads', 'Tails', 'Tails', 'Tails', 'Heads', 'Tails', 'Tails', 'Heads', 'Tails', 'Heads', 'Heads', 'Heads', 'Heads', 'Heads', 'Heads', 'Heads', 'Tails', 'Heads', 'Tails', 'Tails', 'Tails', 'Tails', 'Tails', 'Tails']\n",
      "5545\n"
     ]
    }
   ],
   "source": [
    "lst = coinToss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember its a random experiment so the output will change everytime you run it. Here is the output sequence we'll use in this lesson: \n",
    "```\n",
    "['Heads', 'Heads', 'Tails', 'Tails', 'Tails', 'Heads', 'Tails', 'Heads', 'Heads', 'Heads']\n",
    "```\n",
    "Considering its a random experiment, you can say that there has to be *some* underlying parameter for the outcome of a coin flip. Also, consider other random experiments with dice rolls. Can you identify a parameter that determines the outcome of such experiments ? \n",
    "\n",
    "Parameter Inference is all to do with identifying that parameter with its optimal value. The first key step in this process is Maximum Likelihood Estimation (MLE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "MLE primarily deals with **determining the parameters** that **maximize the probability of the data**. Such a determination can help you predict the outcome of future experiments e.g. If we Toss the coin 1 more time, what is the probability of seeing a Head? \n",
    "\n",
    "* Its a fair coin so probability is 0.5. \n",
    "\n",
    "This is a safe assumption as it assumes independence between coin flips and hence past events have no impact on future ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_head = lst.count(\"Heads\")/10\n",
    "p_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both these approaches in hand , let's see which answer is more suitable by creating a general case from this example. You want to know the probability of 11th flip $ p_{11}(f_{11})$, being a head so you can write:\n",
    "\n",
    "> $p_{11}(f_{11}=Heads)$\n",
    "\n",
    "You can also write above for calculating probability of $ith$ flip being a Head:\n",
    "\n",
    "> $p_i(f_i = Heads) = \\theta_i$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\theta_i$ is the parameter that governs the outcome of ith flip. To signify that the probability distribution depends on $\\theta_i$, you can use conditioning as you saw earlier and write down the last equation to show the probability distribution function along with its dependence on theta_i. \n",
    "\n",
    "> $p_i((f_i = Heads) | \\theta_i)$\n",
    "\n",
    "*The probability of seeing heads in the ith flip , given theta_i*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense so far, but raises a few confusing points: If the data depends on theta parameter, then the first ten coin flips f_1 to f_10 depend on theta_1 to theta_10  for i = 1 to 10. So looking at the outcome of first ten experiments, how can we extrapolate it to theta_11? \n",
    "\n",
    "\n",
    "Here's how you can do this&mdash;if you say that random outcome of a sequence of flips is governed (or modeled) by the parameters theta_1 to theta_10, you can calculate the probability function based on observed data as:\n",
    "\n",
    "> $P (Heads, Heads, Tails, Tails, Tails, Heads, Tails, Heads, Heads, Heads) | \\theta_{1}\\theta_{2} .. \\theta_{10})$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is where Maximum Likelihood Estimation steps into the equation. The problem you have now is that you need to find values of thetas 1 to 10.  MLE helps find theta_i’s such that that probability function shown above is **as high as possible** and this is the basic principle of MLE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood - The probability of data\n",
    "\n",
    "MLE looks at the probability of data and it tries to find those parameters (i.e. theta_1 through theta_10 in above case) that maximize the likelihood of this sequence occurring. \n",
    "\n",
    "> With maximum liklihood estimation, we want to choose those parameters under which our observations become most **likely**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to our coin flip example. If in our understanding, the coin flips do not affect each other, i.e., they are independent (the outcome of first flip does not affect the outcome of the second flip):\n",
    ">$P (H, H, T, T, T, H, T, H, H, H) | \\theta_{1}\\theta_{2} .. \\theta_{10})$\n",
    "\n",
    ">$= P(F_1 = H | \\theta_1).P(F_2 = H | \\theta_2) .. P(F_{10} = H | \\theta_{10})$\n",
    "\n",
    ">$= \\prod_{i=1}^{10} p_i(F_i = f_i | \\theta_i)$ - The general case for coin flip\n",
    "\n",
    "Note: $\\prod$ signifies the product over a series, shown in the previous equation, just as $\\Sigma$ denotes summation over a series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Assumptions\n",
    "\n",
    "Note here that the **independence assumption** allows you to simplify the complex likelihood term into ten simpler factors that can be shown through a general notation in the last equation. \n",
    "\n",
    "The independence assumption allows simplification of the likelihood term but you still don’t have theta_11 in the equation.\n",
    "\n",
    "There is another assumption you can introduce, based on the fact that the coin does not change significantly after each flip i.e.:\n",
    "\n",
    "* **The flips are quantitatively same, i.e., they are identically distributed**. \n",
    "\n",
    "This implies that the flips are taking place under similar circumstances, you can assume that the parameter governing the flips is one and same i.e. just the $\\theta$ without any subscripts. Based on this assumption, you can rewrite above equation as :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">$\\prod_{i=1}^{10} p_i(F_i = f_i | \\theta_i) = \\prod_{i=1}^{10} p(F_i = f_i | \\theta) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assumption leads you to believe that the 10 flips are governed by the same parameter theta. You now have just one parameter governing the entire sequence of coin flips, and that includes the 11th flip as well. \n",
    "\n",
    "This is how MLE allows you to connect first 10 coin flips to the 11th coin flip and is the key for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The two assumptions you made are used so often in Machine Learning that they have a special name together as an entity : \"The i.i.d. assumption\" i.e. Independent and Identically distributed samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the 10 flips are independent and identically distributed which is great as it will allow you to explicitly write down the likelihood that you are trying to optimize. \n",
    "\n",
    "Remember that theta was defined as the probability of the flip showing up Heads; the probability of the sequence w.r.t. theta can now be formulated as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\prod_{i=1}^{10} p(F_i = f_i | \\theta)$ \n",
    "\n",
    "$= \\theta \\theta (1-\\theta)(1-\\theta)(1-\\theta) \\theta(1-\\theta)\\theta\\theta\\theta$\n",
    "\n",
    "$=\\theta^6(1-\\theta)^4$\n",
    "\n",
    "* theta = Probability of seeing a head\n",
    "* 1 - theta = Probability of seeing a tail\n",
    "* The sequence:  H,H,T,T,T,H,T,H,H,H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see here the i.i.d. assumptions simplifies  the likelihood function to a simple polynomial; to a point where you can **start optimizing the function for the parameter theta**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simplified polynomial expression can be interpreted as a function of theta i.e., \n",
    "> $f(\\theta)$ \n",
    "\n",
    "Now you want to find out the maxima (maximum likelihood) of this function. \n",
    "\n",
    "<img src=\"images/der.png\" width= \"200\">\n",
    "\n",
    "\n",
    "Following the intuition in the image above, you can achieve theta this by taking the derivative \n",
    "> $\\frac{df}{d(\\theta)}$ \n",
    "\n",
    "Set this  zero, and solve for theta. Then verify the critical point i.e. maxima, by inserting it into the second derivative of f(theta). This is a simple approach, however, the application of product rule repeatedly in this process could be a technically challenging process. This calculation can be simplified using a monotonic function. \n",
    "\n",
    "\n",
    "### Monotonic function\n",
    "\n",
    "> In mathematics, a monotonic function(or monotone function) is a function between ordered sets that preserves or reverses the given order. This concept first arose in calculus, and was later generalized to the more abstract setting of order theory. [Wiki](https://en.wikipedia.org/wiki/Monotonic_function)\n",
    "\n",
    "\n",
    "\n",
    "According to this theory, if you apply a monotonic function to another function , like the one you are trying to optimize above, this application will preserve the critical points (maxima in this case) of the original function. Logarithmic functions are normally used within the domain of machine learning to achieve the functionality of monotonicity  The logarithmic function is described as:\n",
    "\n",
    "> $log_b(x)$\n",
    "\n",
    "* where b is any number such that b > 0, b≠ 1, and x > 0. \n",
    "* The function is read \"log base b of x\".\n",
    "\n",
    "The logarithm y is the exponent to which b must be raised to get x. The behavior of a log function can be understood from following image.\n",
    "\n",
    "\n",
    "<img src=\"images/new_log.png\" width=\"700\">\n",
    "\n",
    "\n",
    "This helps you realize that **log of f(θ) i.e. log(f(θ)) will have the save maxima as the likelihood function f(θ).** This is better known as the **log likelihood**. \n",
    "\n",
    "Thus, the optimization function i.e. $\\theta^6(1-\\theta)^4$ , that you're trying to optimize w.r.t. theta can be written down as:\n",
    "\n",
    ">$\\underset{\\theta}{\\operatorname{argmax}} \\theta^6(1-\\theta)^4$\n",
    "\n",
    ">In mathematics, the arguments of the maxima (abbreviated arg max or argmax) are the points of the domain of some function at which the function values are maximized\n",
    "\n",
    "Remember that you are not concerned with the actual maximum value of the function. You want to **learn the value for theta** where the **function has the maximum value**.\n",
    "\n",
    "Following the monotonicity principle, the argmax function can be written with natural log *ln* as:\n",
    "\n",
    ">$\\underset{\\theta}{\\operatorname{argmax}} ln(\\theta^6(1-\\theta)^4)$\n",
    " \n",
    "> $=\\underset{\\theta}{\\operatorname{argmax}} 6 (ln (\\theta)) *4 (ln(1-\\theta))$\n",
    "\n",
    "Let's call our log likelihood function $g(\\theta)$, take its derivative and set it to zero. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $ \\frac{d}{d\\theta}[g(\\theta)] = |H|\\frac{1}{\\theta} + |T|\\frac{1}{1-\\theta}(-1)$\n",
    "\n",
    "|T| are the number of tails = 4 \n",
    "|H| are the number of heads = 6 \n",
    "\n",
    "You are simply solving for a general case here , so use |T| and |H|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $|H|\\frac{1}{\\theta} + {|T|}\\frac{1}{1-\\theta}(-1) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $|H|(1-\\theta) - |T|\\theta = 0$\n",
    "\n",
    "> $\\theta = \\frac{|H|}{|H|+ |T|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Maximum Likelihood Function $\\theta_MLE$ for any given sequence of coins. \n",
    "\n",
    "$$\\theta_{MLE} = \\frac{|H|}{|H|+ |T|}$$\n",
    "\n",
    "For the initial problem, where H = 6  and T = 4, you get MLE for theta as 6/10 = 0.6 , or , 60% chance of seeing a head for the 11th coin given the data from first 10 coin flips. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This maximum is called the **MLE for theta** as it makes the observed sequence **most likely**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of MLE\n",
    "\n",
    "Consider a scenario where you get this sequence by total chance: [T,T,T,T,T]. According to the derived MLE formula, the probability of seeing a head at 6th coin toss would be zero. This demonstrates how MLE heavily depends on past data to find the likelihood function. It also indicates that MLE is only a first step for Parameter Estimation. We shall come across more sophisticated approaches like Maximum Aposteriori Estimate (MAP) and Fully Bayesian Analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "This section was pretty math heavy and included many new concepts like optimization, maximas and minimas, monotonicity and log functions. With that, take some time to go through following resources to see more example of MLE calculation and get a deep dive into the underlying mathematical theory. \n",
    "\n",
    "* [Probability Concepts Explained: Maximum Likelihood Estimation](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1) - Example for calculating MLE with normal distributions.\n",
    "* [IID Statistics: Independent and Identically Distributed ](https://www.statisticshowto.datasciencecentral.com/iid-statistics/)\n",
    "* [Monotonically Increasing and Decreasing function: An algebraic approach](https://opencurriculum.org/5512/monotonically-increasing-and-decreasing-functions-an-algebraic-approach/)\n",
    "* [Logarithm Functions](https://mathbitsnotebook.com/Algebra2/Exponential/EXLogFunctions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this lesson, you began to develop an intuition surrounding MLE. You saw how to use the principle of monotonicity to simplify complex probability calculations into simple arithmetic operations. You also looked at a simple example of a coin toss for MLE. You're well on your way to conducting further complex statistical experiments using Bayesian techniques!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
