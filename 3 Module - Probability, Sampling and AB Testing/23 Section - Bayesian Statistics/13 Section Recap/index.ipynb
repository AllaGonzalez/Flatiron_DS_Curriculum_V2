{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics - Recap\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Well done! You covered a lot of ground in this section. From Bayes' theorem to some introductory NLP work, you now have the foundation to dive into the world of Bayesians!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Summarize Bayes' Theorem and potential applications\n",
    "* Discuss various Naive Bayes algorithms\n",
    "* Discuss the difference between Frequentists and Bayesian statistics\n",
    "\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "To start, you investigated Bayes' theorem and some hypothetical examples.\n",
    "\n",
    "$$ \\large P(A|B) = \\dfrac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "## Bayesian Statistics\n",
    "\n",
    "\n",
    "From there, you then went on to read more about some of the philosophical differences between Bayesians and Frequentists. Bayesians interpret probability as the level of confidence or belief in an event. In contrast, Frequentists view probability as the limit as the number of trials goes to infinity of successes versus trials. \n",
    "\n",
    "## MLE and MAP\n",
    "\n",
    "In outlining the discussion of Bayesian techniques, you got an introduction to Maximum Likelihood Estimation and Maximum A Posteriori Estimation. In both, you saw methods for optimizing one's beliefs given certain information. This was used to estimate parameters for assumed distributions.\n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "\n",
    "In moving to machine learning applications, you first investigated the Gaussian Naive Bayes classifier. This produced powerful results when applied to the two small datasets explored. To do this, you viewed each feature as a normal distribution using the mean and standard deviation for a particular subgroup. You then used these to find point estimates along the PDF and fed this through a multinomial Bayes setup.\n",
    "\n",
    "## Document Classification with Naive Bayes\n",
    "\n",
    "After exploring Bayes using normal distributions, you then investigated an offshoot to natural language processing. Due to insufficient preprocessing such as stop words, stemming and lemmatization, the performance of this algorithm was trivial. That said, the concepts should provide you with interesting questions as to other methods for adapting Naive Bayes algorithms to data of different forms.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Again, quite a bit was covered in this section. There is certainly plenty of additional resources available if you wish to further dive into MLE, MAP or other Bayesian techniques. Bayesian inference can provide a powerful a powerful framework for quantifying and reasoning with uncertainty that has continued to gain popularity with additional computing resources. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
